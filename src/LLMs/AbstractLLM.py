import os
import re
import time
from abc import ABC, abstractmethod
from enum import Enum
from typing import List, Literal, Any

import torch
from pydantic import BaseModel
from tqdm import tqdm

from .. data_model import ModelInstantiationError, BasicLLMConfig, SummaryError
from .. Logger import logger

# Definitions below moved to constants.py -- Forrest, 2025-07-02
# MODEL_FAILED_TO_RETURN_OUTPUT = "MODEL FAILED TO RETURN ANY OUTPUT"
# MODEL_RETURNED_NON_STRING_TYPE_OUTPUT = (
#     "DID NOT RECEIVE A STRING TYPE FROM OUTPUT"
# )
# EMPTY_SUMMARY = (
#     "THIS SUMMARY IS EMPTY, THIS IS THE DEFAULT VALUE A SUMMARY "
#     "VARIABLE GETS. A REAL SUMMARY WAS NOT ASSIGNED TO THIS VARIABLE."
# )
# INCOMPLETE_THINK_TAG = "FOUND <think> WITH NO CLOSING </think>"

# SUMMARY_ERRORS = [
#     MODEL_FAILED_TO_RETURN_OUTPUT,
#     MODEL_RETURNED_NON_STRING_TYPE_OUTPUT,
#     EMPTY_SUMMARY,
#     INCOMPLETE_THINK_TAG
# ]

class AbstractLLM(ABC):
    """
    Abstract Class for an LLM.
    """

    def __init__(self, config: BasicLLMConfig) -> None:
        # Expose all config keys and values as attributes on self
        # for key, value in config.model_dump().items():
        #     setattr(self, key, value)

        self.company = config.company
        self.model_name = config.model_name

        # Set defaults for optional attributes
        # self.prompt = config.prompt if config.prompt is not None else default_prompt
        # self.temperature = config.temperature if config.temperature is not None else 0.0
        # self.max_tokens = config.max_tokens if config.max_tokens is not None else 1024
        # self.min_throttle_time = config.min_throttle_time if config.min_throttle_time is not None else 0.1

        self.prompt = config.prompt
        self.temperature = config.temperature
        self.max_tokens = config.max_tokens
        self.min_throttle_time = config.min_throttle_time

        # The following attributes are not required by all models.
        self.date_code = config.date_code       
        self.thinking_tokens = config.thinking_tokens
        self.execution_mode = config.execution_mode

        if self.date_code not in [None, "", " "]:
            self.model_fullname = f"{self.model_name}-{self.date_code}"
        else:
            self.model_fullname = self.model_name

        self.client: Any | None = None # in case the model can be called via web api
        self.local_model: Any | None = None # in case the model can be run locally

        # self.summary_file: str | None = None # won't be set until after instantiation in summarize.py

    def __enter__(self):
        self.setup() # TODO: Try to skip the setup() and teardown() 
        return self

    def __exit__(self, exc_type, exc_val, exc_t):
        self.teardown()

    # Commented out by Forrest, 2025-07-16 
    # Due to not used at all 
    # def summarize_articles(self, articles: list[str]) -> list[str]:
    #     """
    #     Takes in a list of articles, iterates through the list. Returns list of
    #     the summaries

    #     Args:
    #         articles (list[str]): List of strings where the strings are human
    #         written news articles

    #     Returns:
    #         list[str]: List of articles generated by the LLM
    #     """
    #     summaries = []
    #     for article in tqdm(articles, desc="Article Loop"):
    #         summary = self.summarize_clean_wait(article)
    #         summaries.append(summary)
    #     return summaries

    def try_to_summarize_one_article(self, article: str) -> str:
        """
        Tries to request the model to summarize an Article. Logs warnings if it
        fails but continues the program with dummy output indicative of the 
        failure

        Args:
            Article (str): Article to be summarized

        Returns:
            str: Summary of article or error message
        
        """
        # llm_summary = self.summary_error.EMPTY_SUMMARY
        # This line not needed. -- Forrest, 2025-07-02

        # prepared_llm_input = self.prepare_article_for_llm(article)

        prepared_llm_input = self.prompt.format(article=article)

        start_time = time.time()

        try:
            llm_summary = self.summarize(prepared_llm_input)
        except Exception as e:
            logger.warning((
                f"Model call failed for {self.model_name}: {e} "
            ))
            return SummaryError.MODEL_FAILED_TO_RETURN_OUTPUT

        if not isinstance(llm_summary, str):
            bad_output = llm_summary
            logger.warning((
                f"{self.model_name} returned unexpected output. Expected a "
                f"string but got {type(bad_output).__name__}. "
                f"Replacing output."
            ))
            return SummaryError.MODEL_RETURNED_NON_STRING_TYPE_OUTPUT
        
        llm_summary = self.remove_thinking_text(llm_summary)

        elapsed_time = time.time() - start_time
        remaining_time = self.min_throttle_time - elapsed_time
        if remaining_time > 0:
            time.sleep(remaining_time)

        return llm_summary

    def remove_thinking_text(self, raw_summary: str) -> str:
        """
        Removes any thinking tags and content in between them. If a summary does
        not have a closing thinking tag it will be considered as an incomplete 
        summary and return an error string instead.

        TODO: Note that different LLMs may use different tags for thinking. Shall we move to child classes?

        Args:
            raw_summary (str): raw summary from LLM

        returns:
            str: summary without thinking data or invalid summary text

        """
        if '<think>' in raw_summary and '</think>' not in raw_summary:
            logger.warning(f"<think> tag found with no </think>. This is indicative of an incomplete response from an LLM. Raw Summary: {raw_summary}")
            return SummaryError.INCOMPLETE_THINK_TAG

        summary = re.sub(
            r'<think>.*?</think>\s*', '',
            raw_summary, flags=re.DOTALL
        )
        return summary

    def default_local_model_teardown(self):
        """
        Standard protcol for tearing down a torch based model, Sets 
        self.local_model to None when done

        Args:
            None
        Returns:
            None
        """
        self.local_model.to("cpu")
        del self.local_model
        torch.cuda.empty_cache()
        self.local_model = None

    def prepare_for_overwrite(self, summaries_jsonl_path: str):
        """
        Prepare for overwriting existing summaries
        """
        if self.date_code in [None, "", " "]:
            # Clean the summary file 
            open(summaries_jsonl_path, 'w').close()
        else: 
            # Remove summaries in existing summary file that match the model name, date code, and summary_date
            df = pd.read_json(summaries_jsonl_path, lines=True)
            df = df[(df['model_name'] != self.model_name) | (df['date_code'] != self.date_code) | (df['summary_date'] != self.eval_date)]
            df.to_json(summaries_jsonl_path, orient='records', lines=True)

    @abstractmethod
    def summarize(self, prepared_text: str) -> str:
        """
        Requests LLM to generate a summary given the input

        Args:
            prepared_text (str): Prompt prepared text

        Returns:
            str: Generated LLM summary
        """
        return None

    @abstractmethod
    def setup(self):
        """
        Setup model for use

        Args:
            None
        Returns:
            None
        """
        return None

    @abstractmethod
    def teardown(self):
        """
        Teardown model

        Args:
            None
        Returns:
            None
        """
        return None

    @abstractmethod
    def close_client(self):
        """
        Close client

        Args:
            None
        Returns:
            None
        """
        return None