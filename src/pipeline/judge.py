"""Judgment pipeline for evaluating LLM-generated summaries.

This module provides functionality for scoring summaries against source articles
using hallucination detection models. Iterates through summaries generated by
configured LLMs and produces judgment scores indicating factual consistency.

The pipeline primarily uses Vectara's HHEM (Hughes Hallucination Evaluation Model)
family (versions 2.1-open, 2.3, etc.). Additionally supports HDM-2, an external
open-source hallucination detection model from a separate research group, which
can be used for comparative experiments.

Functions:
    get_hhem_model: Factory function to instantiate hallucination detection models.
    get_judgments: Main entry point for generating judgment scores.
    generate_judgments: Core judgment generation for a single LLM's summaries.
"""

import os
from datetime import datetime, timezone
from typing import Literal
import pandas as pd
from tqdm import tqdm

from .. analytics import is_valid_summary
from .. data_model import BasicJudgment, SourceArticle, EvalConfig, BasicSummary
from .. HHEM_2_x import HHEM_2_1_open, HHEM_2_3, HHEMOutput, HHEM_2_3_PROD, HHEM_2_3_API
from .. HDM_2 import HDM2
from .. json_utils import append_record_to_jsonl
from .. Logger import logger

def get_hhem_model(
        hhem_version: Literal["2.1-open", "2.3", "HDM-2", "2.3-PROD", "2.3-API"]
    ):
    """Factory function to instantiate the appropriate hallucination detection model.

    Creates and returns an instance of a hallucination detection model based
    on the specified version string. Primarily supports Vectara's HHEM family,
    but also includes HDM-2 for comparative experiments.

    Args:
        hhem_version: Version identifier for the model. Supported values:
            - "2.1-open": Open-source HHEM 2.1 model (Vectara).
            - "2.3": Standard HHEM 2.3 model (Vectara).
            - "2.3-PROD": Production variant of HHEM 2.3 (Vectara).
            - "2.3-API": API-based HHEM 2.3 model (Vectara).
            - "HDM-2": External open-source hallucination detection model
                from a separate research group, used for comparison.

    Returns:
        An initialized model instance with a predict() method.

    Raises:
        ValueError: If an unsupported version is specified.

    Note:
        HDM-2 is not part of the HHEM family. It is an external open-source
        model included to enable comparative evaluation experiments.
    """
    if hhem_version == "2.1-open":
        return HHEM_2_1_open()
    elif hhem_version == "2.3":
        return HHEM_2_3()
    elif hhem_version == "2.3-PROD":
        return HHEM_2_3_PROD()
    elif hhem_version == "2.3-API":
        return HHEM_2_3_API()
    elif hhem_version == "HDM-2": # This is not a HHEM model
        return HDM2()
    else:
        raise ValueError(f"Unsupported HHEM version: {hhem_version}. Supported versions: 2.1-open, 2.3")

def get_judgments(eval_config: EvalConfig, article_df: pd.DataFrame):
    """Generate judgment scores for summaries produced by all configured LLMs.

    Main entry point for the judgment pipeline. Iterates through all LLM
    configurations, loads their generated summaries, and produces HHEM
    judgment scores for each summary-article pair. Results are saved
    incrementally to JSONL files.

    Args:
        eval_config: Evaluation configuration containing LLM configs, HHEM
            version, output paths, and file naming settings.
        article_df: DataFrame containing source articles with article IDs
            and text content for comparison against summaries.

    Note:
        Skips LLMs whose summary files are not found, logging a warning
        for each missing file.
    """
    summary_file = eval_config.summary_file
    judgment_file = eval_config.judgment_file


    LLMs_to_be_processed = [llm_config.model_name for llm_config in eval_config.per_LLM_configs]
    logger.info(f"Starting to generate {judgment_file} scores for the following LLMs: {LLMs_to_be_processed}")

    for llm_config in tqdm(eval_config.per_LLM_configs, desc="LLM Loop"):
        model_name = llm_config.model_name

        llm_alias = ""
        if llm_config.date_code == "" or llm_config.date_code == None:
            llm_alias = f"{llm_config.model_name}"
        else:
            llm_alias = f"{llm_config.model_name}-{llm_config.date_code}"
        
        model_out_dir = ""
        if llm_config.date_code == "" or llm_config.date_code == None:
            model_out_dir = os.path.join(
                eval_config.output_dir, 
                llm_config.company, 
                model_name
            )
        else:
            model_out_dir = os.path.join(
                eval_config.output_dir, 
                llm_config.company, 
                f"{llm_config.model_name}-{llm_config.date_code}"
            )
        summaries_jsonl_path = os.path.join(model_out_dir, summary_file)

        logger.info(f"Generating judgment file {judgment_file} for LLM {llm_alias}")

        if os.path.isfile(summaries_jsonl_path):
            summaries_df = pd.read_json(summaries_jsonl_path, lines=True)
            
            judgments_jsonl_path = os.path.join(model_out_dir, judgment_file)
            open(judgments_jsonl_path, 'w').close()
            
            hhem_model = get_hhem_model(eval_config.hhem_version)
            article_summary_df = pd.merge(
                article_df, summaries_df,
                on=BasicSummary.Keys.ARTICLE_ID, how='inner'
            )
            generate_judgments(
                hhem_model, article_summary_df, judgments_jsonl_path, eval_config.eval_name, eval_config.eval_date
            )
            logger.info(f"Finished judging summaries produced by LLM {llm_alias} and saved to {judgment_file}")
        else:
            logger.warning(
                f"Summary file {summaries_jsonl_path} not found for LLM {llm_alias}, skipping LLM"
            )
    logger.info(f"Finished generating {judgment_file} scores for the following LLMs: {LLMs_to_be_processed}")

def generate_judgments(
    hhem_model: HHEM_2_1_open | HHEM_2_3,
    article_summary_df: pd.DataFrame,
    judgments_jsonl_path: str,
    eval_name: str,
    eval_date: str,
):
    """Generate judgment scores for summaries using an HHEM model.

    Iterates through each article-summary pair, computes the HHEM score
    indicating factual consistency, validates the summary, and saves
    the judgment record incrementally to a JSONL file.

    Args:
        hhem_model: Initialized HHEM or HDM model instance for scoring.
            Must have a predict() method returning an HHEMOutput with score.
        article_summary_df: DataFrame containing merged article and summary
            data with columns for article text, summary text, and summary UID.
        judgments_jsonl_path: Path to the output JSONL file for judgments.
        eval_name: Name identifier for this evaluation run.
        eval_date: Date identifier for this evaluation run.

    Note:
        Each judgment record includes the HHEM score, validity flag,
        word count, and evaluation metadata.
    """
    for _, row in tqdm(article_summary_df.iterrows(), total=len(article_summary_df),
                       desc="HHEM/Judgment Loop"):
        hypothesis = row[BasicSummary.Keys.SUMMARY]
        word_count = len(hypothesis.split())
        valid_summary = is_valid_summary(hypothesis)
        metric_record = BasicJudgment(
            eval_name=eval_name,
            judgment_date=eval_date,
            summary_uid=row[BasicSummary.Keys.SUMMARY_UID],
            hhem_version=hhem_model.__str__(),
            hhem_score=hhem_model.predict(row[SourceArticle.Keys.TEXT], hypothesis).score,
            is_valid=valid_summary,
            word_count = word_count
        )
        append_record_to_jsonl(judgments_jsonl_path, metric_record)

if __name__ == "__main__":
    pass