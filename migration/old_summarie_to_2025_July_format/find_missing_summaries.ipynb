{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01b397d5",
   "metadata": {},
   "source": [
    "The summaries in https://huggingface.co/datasets/vectara/leaderboard_results do not cover all LLMs that we have evaluated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a5a703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "import json, os\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset, Dataset\n",
    "from huggingface_hub import Repository\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e14d79a",
   "metadata": {},
   "source": [
    "# 1. Load summaries from the HF summary repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1c0f4e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = load_dataset(\"vectara/leaderboard_results\", split=\"train\")\n",
    "models_at_hf_summaries=set(summaries['model'])\n",
    "models_at_hf_summaries = set([x.lower() for x in models_at_hf_summaries])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4421fe6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['openai/gpt-4o-mini',\n",
       " 'openai/gpt-4.5-preview',\n",
       " 'openai/gpt-4.1-nano',\n",
       " 'openai/gpt-4.1-mini',\n",
       " 'openai/gpt-4o',\n",
       " 'openai/chatgpt-4o-latest',\n",
       " 'openai/gpt-4-turbo-2024-04-09',\n",
       " 'openai/gpt-4',\n",
       " 'openai/gpt-3.5-turbo',\n",
       " 'openai/gpt-4.1']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in models_at_hf_summaries if \"gpt\" in x.lower()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5e504a",
   "metadata": {},
   "source": [
    "# 2. Load eval stats from HF results repo and get LLMs whose summaries are missing in summaries repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e30982",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/forrest/.local/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.\n",
      "For more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "/home/forrest/github_hallucination_leaderboard/migration/old_summarie_to_2025_July_format/./results is already a clone of https://huggingface.co/datasets/vectara/results. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully pulled results from ./results\n",
      "Successfully scanned and extracted results from ./results and saved to ./results.json\n"
     ]
    }
   ],
   "source": [
    "# Load Previous evaluation results\n",
    "\n",
    "def pull_results(results_dir: str):\n",
    "    repo = Repository(local_dir = results_dir, clone_from=\"vectara/results\", repo_type=\"dataset\") \n",
    "    repo.git_pull()\n",
    "\n",
    "def extract_info_from_result_file(result_file):\n",
    "    \"\"\"\n",
    "        {\n",
    "        \"config\": {\n",
    "            \"model_dtype\": \"float16\",\n",
    "            \"model_name\": \"databricks/dbrx-instruct\",\n",
    "            \"model_sha\": \"main\"\n",
    "        },\n",
    "        \"results\": {\n",
    "            \"hallucination_rate\": {\n",
    "            \"hallucination_rate\": 8.34990059642147\n",
    "            },\n",
    "            \"factual_consistency_rate\": {\n",
    "            \"factual_consistency_rate\": 91.65009940357854\n",
    "            },\n",
    "            \"answer_rate\": {\n",
    "            \"answer_rate\": 100.0\n",
    "            },\n",
    "            \"average_summary_length\": {\n",
    "            \"average_summary_length\": 85.9\n",
    "            }\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    info = json.load(open(result_file, 'r'))\n",
    "    result = {\n",
    "        \"LLM\": info[\"config\"][\"model_name\"],\n",
    "        \"Hallucination %\": info[\"results\"][\"hallucination_rate\"][\"hallucination_rate\"],\n",
    "        # \"Factual Consistency Rate\": info[\"results\"][\"factual_consistency_rate\"][\"factual_consistency_rate\"],\n",
    "        \"Answer %\": info[\"results\"][\"answer_rate\"][\"answer_rate\"],\n",
    "        \"Avg Summary Words\": info[\"results\"][\"average_summary_length\"][\"average_summary_length\"],\n",
    "    }\n",
    "    return result\n",
    "\n",
    "def get_latest_result_file(dir: str):\n",
    "    \"\"\"\n",
    "        Get the latest result file in the given directory based on the timestamp in the file name.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(dir):\n",
    "        return None\n",
    "    files = os.listdir(dir)\n",
    "    files = [f for f in files if f.endswith(\".json\")]\n",
    "    if len(files) == 0:\n",
    "        return None\n",
    "    files.sort(key=lambda x: os.path.getmtime(os.path.join(dir, x)))\n",
    "    # print (\"Scanning: \", dir, \"found latest file: \", files[0])\n",
    "    return os.path.join(dir, files[0])\n",
    "\n",
    "def scan_and_extract(dir: str):\n",
    "    \"\"\"Scan all folders recursively and exhaustively to load all JSON files and call `extract_info_from_result_file` on each one.\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "    for root, dirs, files in os.walk(dir):\n",
    "        if len(dirs) == 0:\n",
    "            continue\n",
    "        for dir in dirs:\n",
    "            result_file = get_latest_result_file(os.path.join(root, dir))\n",
    "            if result_file is not None:\n",
    "                results.append(extract_info_from_result_file(result_file))\n",
    "    return results\n",
    "\n",
    "def load_results(\n",
    "        results_dir: str = \"./results\", \n",
    "        results_json: str = \"./results.json\"\n",
    "        ):\n",
    "    \n",
    "    try: \n",
    "        pull_results(results_dir)\n",
    "        print (f\"Successfully pulled results from {results_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to pull and/or extract latest results: {e}\")\n",
    "    \n",
    "    try: \n",
    "        results = scan_and_extract(results_dir)\n",
    "        if len(results) > 0:\n",
    "            with open(results_json, \"w\") as f:\n",
    "                json.dump(results, f, indent=2)\n",
    "            print(f\"Successfully scanned and extracted results from {results_dir} and saved to {results_json}\")\n",
    "        else:\n",
    "            print(f\"No results found in {results_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to scan and extract results from {results_dir}: {e}\")\n",
    "        print(f\"Using pre-dumped results from {results_json}\")\n",
    "\n",
    "    results = json.load(open(results_json, \"r\"))\n",
    "    # print(results)\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = results_df.sort_values(by=\"Hallucination %\", ascending=True)\n",
    "\n",
    "    # replace any value TBD with -1\n",
    "    results_df = results_df.replace(\"TBD\", 100)\n",
    "\n",
    "    for column in [\"Hallucination %\", \"Answer %\", \"Avg Summary Words\"]:\n",
    "        results_df[column] = results_df[column].apply(lambda x: round(x, 3))\n",
    "\n",
    "    results_df[\"LLM_lower_case\"] = results_df[\"LLM\"].str.lower()\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "stats_df = load_results() #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f381b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anthropic/claude-3-5-sonnet',\n",
       " 'anthropic/claude-4-opus',\n",
       " 'anthropic/claude-4-sonnet',\n",
       " 'deepseek/deepseek-r1-0528',\n",
       " 'google/gemini-2.5-pro-preview-06-05',\n",
       " 'google/gemma-3-12b-it',\n",
       " 'moonshotai/kimi-k2-instruct',\n",
       " 'openai/gpt-4-turbo',\n",
       " 'openai/o3-mini-high-reasoning',\n",
       " 'vectara/mockingbird-2-echo',\n",
       " 'xai/grok-4-0709'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models_at_hf_stats = set(stats_df[\"LLM_lower_case\"].to_list())\n",
    "models_at_hf_stats = set([x.lower() for x in models_at_hf_stats])\n",
    "\n",
    "# find models in models_at_hf_stats that are not in models_at_hf_summaries using set subtraction\n",
    "\n",
    "models_missing_summaries = models_at_hf_stats - models_at_hf_summaries\n",
    "display (models_missing_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cef290",
   "metadata": {},
   "source": [
    "Note that some models above are \"missing\" because they are evaluated using the new framework and are not pushed to HF summary repo `leaderboard_results` which is to be phased out. \n",
    "\n",
    "### Now let's confirm that they are really missing, not due to a different name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22877db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['openai/gpt-4o-mini',\n",
       " 'openai/gpt-4.5-preview',\n",
       " 'openai/gpt-4.1-nano',\n",
       " 'openai/gpt-4.1-mini',\n",
       " 'openai/gpt-4o',\n",
       " 'openai/chatgpt-4o-latest',\n",
       " 'openai/gpt-4-turbo-2024-04-09',\n",
       " 'openai/gpt-4',\n",
       " 'openai/gpt-3.5-turbo',\n",
       " 'openai/gpt-4.1']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check similar names in models_at_hf_summaries\n",
    "[x for x in models_at_hf_summaries if \"gpt\" in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45bd19c",
   "metadata": {},
   "source": [
    "### Discovery: `gpt-4-turbo` in HF_stats should have been `openai/gpt-4-turbo-2024-04-09`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3c0062a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['google/gemma-1.1-2b-it',\n",
       " 'google/gemma-3-27b-it',\n",
       " 'google/gemma-2-2b-it',\n",
       " 'google/gemma-1.1-7b-it',\n",
       " 'google/gemma-3-1b-it',\n",
       " 'google/gemma-7b-it',\n",
       " 'google/gemma-3-4b-it',\n",
       " 'google/gemma-2-9b-it']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in models_at_hf_summaries if \"gemma\" in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1972c49",
   "metadata": {},
   "source": [
    "### Discovery: `gemma-3-12b-it` is really missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ae3977c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['google/gemini-2.0-pro-exp-02-05',\n",
       " 'google/gemini-2.5-flash-preview-04-17',\n",
       " 'google/gemini-1.5-flash-001',\n",
       " 'gemini-2.0-flash-exp',\n",
       " 'google/gemini-2.0-flash-001',\n",
       " 'google/gemini-1.5-flash',\n",
       " 'google/gemini-1.5-pro-002',\n",
       " 'google/gemini-flash-experimental',\n",
       " 'google/gemini-pro-experimental',\n",
       " 'google/gemini-2.5-pro-exp-03-25',\n",
       " 'google/gemini-pro',\n",
       " 'google/gemini-2.0-flash-thinking-exp',\n",
       " 'google/gemini-1.5-pro-001',\n",
       " 'google/gemini-2.0-flash-lite-preview-02-05',\n",
       " 'google/gemini-1.5-pro',\n",
       " 'google/gemini-1.5-flash-002']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in models_at_hf_summaries if \"gemini\" in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fc395a",
   "metadata": {},
   "source": [
    "### Discovery: `google/gemini-2.5-pro-preview-06-05` is really missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "20ef740c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anthropic/claude-3-5-haiku-20241022',\n",
       " 'anthropic/claude-3-7-sonnet-latest-think',\n",
       " 'anthropic/claude-3-7-sonnet-latest',\n",
       " 'anthropic/claude-3-5-sonnet-20241022',\n",
       " 'anthropic/claude-3-sonnet',\n",
       " 'anthropic/claude-2',\n",
       " 'anthropic/claude-3-opus',\n",
       " 'anthropic/claude-3-5-sonnet-20240620']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in models_at_hf_summaries if \"claude\" in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d8d742",
   "metadata": {},
   "source": [
    "### Discovery: `anthropic/claude-3-5-sonnet` is not missing. Just that it had date-code in the name in HF summary repo `leaderboard_results`. the other two missing ones are produced using new framework. So not in HF summary repo `leaderboard_results` as expected. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29987010",
   "metadata": {},
   "source": [
    "## So the actually missing ones are `gemma-3-12b-it` and `google/gemini-2.5-pro-preview-06-05` We will find them in July 2024 Google Drive snapshot of summaries below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6b3e27",
   "metadata": {},
   "source": [
    "# 3. Load from July 2024 Google Drive snapshot of summaries to see whether missing models' summaies are present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d6d2b286",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdrive_summaries = pd.read_csv(\"gdrive_summaries_july_2024.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ddaa42f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['claude',\n",
       " 'claude3-opus',\n",
       " 'Anthropic/claude-3-5-sonnet-20240620',\n",
       " 'claude3-sonnet']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in set(gdrive_summaries['model']) if \"claude\" in x.lower()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531dee6a",
   "metadata": {},
   "source": [
    "### Discovery 2: Google Gemini-2.5-pro-preview-06-05 is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "362d7306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['google/Gemini-1.5-Pro', 'google/Gemini-1.5-flash', 'Google Gemini Pro']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in set(gdrive_summaries['model']) if \"gemini\" in x.lower()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f203cf",
   "metadata": {},
   "source": [
    "### Discovery 3: Gemma-3-12b-it still missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "571c9899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['google/gemma-1.1-2b-it',\n",
       " 'google/gemma-2-9b-it',\n",
       " 'gemma-7b-it',\n",
       " 'google/gemma-1.1-7b-it']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in set(gdrive_summaries['model']) if \"gemma\" in x.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f5c4fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
