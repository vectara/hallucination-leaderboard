# How to run and expand this code base

DO NOT PUSH judgments.jsonl to the repo. It will allow people to chat on the leaderboard.

TODO: 
1. Allow interrupting the program and resume from the same point.
2. Supporting selecting to run HHEM on CPU. 
3. Do we still need test files?
4. update the app code to read from new stats files or shall we have a script that pushes to HF datasets repo?
5. Add a script to generate leaderboard ranking markdown. 
6. Do we still need `model_fullname` ?

## Installation

Suppose you are in the root directory of the project.

```bash
pip install -e .
```

Then, you can use the command `hhem-leaderboard` to run the evaluation. See details below. 

## Files

```plaintext
src/
├── config.py
├── data_model.py
├── LLMs/
│   ├── AbstractLLM.py
│   ├── Anthropic.py
│   ├── OpenAI.py
│   ├── ...
├── pipeline/
│   ├── summarize.py
│   ├── judge.py
│   ├── aggregate.py
├── main.py
├── ...
```

## Classes 

Under `src/LLMs/`, for each LLM provider, there is a corresponding file (`src/LLMs/Anthropic.py`, `src/LLMs/OpenAI.py`, etc.) that contains the implementation of the LLM. Each provider has three classes in heritied from the `AbstractLLM`, `BasicLLMConfig`, and `BasicSummary` classes: 

| Class | Parent | Description |
|-------|--------|-------------|
| `{Provider_name}LLM` | `AbstractLLM` in `src/LLMs/AbstractLLM.py` | The class for all LLMs of the provider. Must have the `summarize`, `setup`, and `teardown` methods. |
| `{Provider_name}Config` | `BasicLLMConfig` in `src/data_model.py` | The parameters for using LLMs of the provider to summarize an article. Because the generation of summaries is provider-specific (e.g., different tags for thinking), their members differ while common ones such as `model_name`, `company`, `temperature` and `max_tokens` are inherited from `BasicLLMConfig`. |
| `{Provider_name}Summary` | `BasicSummary` in `src/data_model.py` | The class for summaries generated by LLMs of the provider. It usually contains provider-specific fields. |

## The configuration file

All setting of evaluations are stored in `src/config.py` which contains one variable `eval_configs` that is a list of `EvalConfig` (defined in `src/data_model.py`) objects. Briefly, an `EvalConfig` object includes but is not limited to the following fields:

- `eval_name`: The name of the evaluation.
- `eval_date`: The date of the evaluation.
- `hhem_version`: The version of HHEM to use.
- `pipeline`: Elements of the evaluation pipeline, which is a list of strings that can only take values from the set `{"summarize", "judge", "aggregate"}`. (Default: `['summarize', 'judge', 'aggregate']` all three steps)
- `overwrite_summaries`: Whether to overwrite the existing summaries. When `True`, current behavior is that summaries that match the model name, date code, and summary date will be removed. (Default: `False`)
- `source_article_path`: The file path to the source articles to be summarized by LLMs under the evaluation. (Default: `datasets/test_articles.csv` which is the test data.)
- `common_LLM_config`: Summarization configurations for all LLMs in this evaluation. (Default: `BasicLLMConfig` with default values)
- `per_LLM_configs`: LLMs covered in this evaluation, each of which is an `{Provider_name}Config` object of the corresponding `{Provider_name}Config` class.

A `BasicLLMConfig` object includes but is not limited to the following fields (not all are required):

- `model_name`: The name of the LLM. 
- `company`: The company that provides the LLM.
- `date_code`: The date code of the LLM. (Default: `None`)
- `prompt`: The prompt to be used for summarization. (Default: a default prompt defined in `src/data_model.py`)
- `temperature`: The temperature to be used for summarization. (Default: `0.0`)
- `max_tokens`: The maximum number of tokens to be used for summarization. (Default: `4096`)
- `min_throttle_time`: The minimum time to wait between requests. (Default: `0.1`)
- `thinking_tokens`: The number of tokens allocated for thinking. (Default: `None`)
- `execution_mode`: The execution mode of the LLM. (Default: `None`)

### Order of supersedes in LLM configs

There are many places that a user can specify the parameters of an LLM when it summarizes an article. The order of supersedes is determined by the following order (top to bottom): 

1. Those in `LLM_Configs` in an `EvalConfig` object in `src/config.py` -- specific for an LLM in an evaluation run. 
2. Those not in `LLM_Configs` in an `EvalConfig` object in `src/config.py` -- default for all LLMs in all evaluation runs.
3. The default values in `{Provider_name}Config` class.
4. The default values in `BasicLLMConfig` class.

## The pipeline

There are three steps in the pipeline defined in `src/pipeline/{summarize, judge, aggregate}.py`: 

1. `summarize`: Generate summaries of for all articles in the dataset specified in `source_article_path` in the `EvalConfig` object. The `source_article_path` is a path to a CSV file of four columns: `article_id`, `text`, `dataset`. See `datasets/test_articles.csv` for an example.
2. `judge`: Get the HHEM score along with the validity and word count of each summary produced by the LLMs specified in `LLM_Configs`.
3. `aggregate`: Aggregate the results to get the hallucination rate, average word count, and answer rate of each LLM specified in `LLM_Configs`.

You do not have to do all three steps together. You can put them in different `EvalConfig` objects and run them in different runs. 

## How to reproduced results

All previous settings are stored in `src/config.py`. To reproduced results, you can just run the command:

```bash
hhem-leaderboard --eval_name <eval_name>
```
where `<eval_name>` is the name of the evaluation you want to re-run. 

## How to contribute by adding a new LLM

### If the LLM is from a provider already supported

To support a new LLM from a provider already supported, just two steps: 

1. Expand the `{Provider_name}LLM` class in `src/LLMs/{Provider_name}.py` to support summarizing using the new LLM. 
2. Create a new entry in `src/config.py` under `eval_configs` with the new LLM.

### If the LLM is from a new provider

To support a new provider, 
1. Create a new file in `src/LLMs/` with the provider's name. Add at least the following classes: 
    - `{Provider_name}Config`: The configuration for the new LLM.
    - `{Provider_name}Summary`: The summary for the new LLM.
    - `{Provider_name}LLM`: The LLM for the new provider which must have the `summarize`, `setup`, and `teardown` methods.
2. Add the new provider to the `MODEL_REGISTRY` in `src/LLMs/__init__.py`.
3. Perform the steps for "If the LLM is from a provider that is already supported" above.